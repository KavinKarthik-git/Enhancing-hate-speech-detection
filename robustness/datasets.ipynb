{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the entire TSV file into a DataFrame\n",
        "file_path = '/content/hx_train.tsv'\n",
        "data = pd.read_csv(file_path, sep='\\t')\n",
        "data['language'] = 'english'\n",
        "modified_hx_train = data\n",
        "modified_hx_train.to_csv('train_hx_modified.tsv',sep='\\t', index=False )\n",
        "\n",
        "# Extract the first 200 rows\n",
        "first_500_rows = data.head(500)\n",
        "\n",
        "# Save the first 200 rows to a new TSV file\n",
        "first_500_rows.to_csv('test_hx_modified.tsv', sep='\\t', index=False)"
      ],
      "metadata": {
        "id": "4EM9zIqYqA7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vbSC6DOlpk2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Robust 1.Take the sentences with word white and change to black and see if it changes the result. Take the sentences with word jew and change to australian and see if it changes the result.\n",
        "**bold text**"
      ],
      "metadata": {
        "id": "c-vYKDs_rrDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'test_hx_modified.tsv'  # Adjust the file path as necessary\n",
        "data = pd.read_csv(file_path, sep='\\t')\n",
        "\n",
        "# Replace 'white' with 'black'\n",
        "data['post'] = data['post'].str.replace('white', 'black', case=False)\n",
        "\n",
        "# Replace 'jew' with 'Australian'\n",
        "data['post'] = data['post'].str.replace('jew', 'Australian', case=False)\n",
        "\n",
        "# Save the modified dataset if needed\n",
        "data.to_csv('test_approach1_invariance.tsv', sep='\\t', index=False)\n"
      ],
      "metadata": {
        "id": "T3j0p840jjjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Robust 2: combing words"
      ],
      "metadata": {
        "id": "3tBuE086r468"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def combine_word(sentence, target_words):\n",
        "    words = sentence.split()\n",
        "    for i in range(1, len(words)):\n",
        "        if words[i].lower() in [word.lower() for word in target_words]:\n",
        "            words[i-1:i+1] = [''.join(words[i-1:i+1])]\n",
        "            break\n",
        "    return ' '.join(words)\n",
        "\n",
        "dataset_path = '/content/test_hx_modified.tsv'\n",
        "df = pd.read_csv(dataset_path, sep='\\t', header=None, names=['post', 'label', 'language'])\n",
        "\n",
        "target_words = ['nigger', 'negroes', 'niggas']\n",
        "df['post'] = df['post'].apply(lambda x: combine_word(x, target_words))\n",
        "\n",
        "df.to_csv('test_approach2.tsv', sep='\\t', index=False)\n"
      ],
      "metadata": {
        "id": "qHWydg4Fpl3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OP8qc-QEM3oY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "!pip install fuzzywuzzy python-Levenshtein\n",
        "\n",
        "dataset_path = '/content/test_hx_modified.tsv'\n",
        "df = pd.read_csv(dataset_path, sep='\\t', header=None, names=['post', 'label','language'])\n",
        "from fuzzywuzzy import process\n",
        "import random\n",
        "\n",
        "variations_dict = {\n",
        "    'bitch': ['beech', 'bi**h', 'b!tch'],\n",
        "    'nigger': ['ni**er', 'n!gger', 'nigg3r'],\n",
        "    'niggers': ['ni**er', 'n!gger', 'nigg3r'],\n",
        "    'slut': ['$lut', 'sl*t', 's1ut'],\n",
        "    'fuck': ['f**k', 'f*ck', 'f@ck', 'f#ck', 'phuck'],\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "def introduce_variation(word, variations_dict):\n",
        "    if word.lower() in variations_dict:\n",
        "        variations = variations_dict[word.lower()]\n",
        "        return random.choice(variations)\n",
        "    return word\n",
        "\n",
        "def modify_sentence(sentence, variations_dict):\n",
        "    words = sentence.split()\n",
        "    new_words = [introduce_variation(word, variations_dict) for word in words]\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "df['post'] = df['post'].apply(lambda x: modify_sentence(x, variations_dict))\n",
        "df = df.drop(df.index[0])\n",
        "\n",
        "df.to_csv('test_approach3.tsv', sep='\\t', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xXSXQ26M4B9",
        "outputId": "363d7867-681c-47c5-e724-d5136dfab239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.23.0-py3-none-any.whl (9.4 kB)\n",
            "Collecting Levenshtein==0.23.0 (from python-Levenshtein)\n",
            "  Downloading Levenshtein-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=3.1.0 (from Levenshtein==0.23.0->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fuzzywuzzy, rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.23.0 fuzzywuzzy-0.18.0 python-Levenshtein-0.23.0 rapidfuzz-3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approach 4 :Directional expectation"
      ],
      "metadata": {
        "id": "JfToI6Z8qTJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'test_hx_modified.tsv'  # Adjust the file path as necessary\n",
        "data = pd.read_csv(file_path, sep='\\t')\n",
        "\n",
        "# Append 'Thank you.‚ù§Ô∏èü§ó' to each entry in the 'post' column\n",
        "data['post'] = data['post'].apply(lambda x: x + ' Thank you.‚ù§Ô∏èü§ó')\n",
        "\n",
        "# Save the modified dataset\n",
        "data.to_csv('test_approach4_directional_expectation.tsv', sep='\\t', index=False)\n"
      ],
      "metadata": {
        "id": "tCAlK3FNqSSd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}